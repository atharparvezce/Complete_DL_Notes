{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ec97ca-3823-4fe8-87a7-2ccd65f37119",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ✅ **What are CNNs**\n",
    "\n",
    "---\n",
    "\n",
    "CNN (Convolutional Neural Network) is a type of deep learning model designed to process grid-like data, especially images.\n",
    "\n",
    "- Images = 2D grids of pixels (e.g., 28×28 grayscale or 224×224×3 RGB).\n",
    "- CNNs use convolutional layers to extract spatial features like edges, textures, shape\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# ✅ **Why Not Use ANN for Images?**\n",
    "\n",
    "## 1. Too Many Parameters\n",
    "\n",
    "- Example: 224×224 RGB image = 150,528 inputs  \n",
    "- ANN with 1 hidden layer of 1,000 neurons:  \n",
    "  → 150,528 × 1,000 = **150 million weights**  \n",
    "  → High memory usage, slow training, overfitting\n",
    "\n",
    "## 2. No Spatial Awareness\n",
    "\n",
    "- ANN flattens the image → loses **spatial structure**\n",
    "- Nearby pixels (edges, shapes) are treated as **independent**\n",
    "\n",
    "---\n",
    "\n",
    "##  Why CNNs Work Better\n",
    "### 1. Convolution Layer\n",
    "\n",
    "- Applies small filters (e.g., 3×3) across the image\n",
    "- Captures **local patterns** like edges and textures\n",
    "- **Shared weights** → fewer parameters\n",
    "\n",
    "\n",
    "### 2. Pooling Layer\n",
    "\n",
    "- Reduces spatial size (e.g., 2×2 max pooling)\n",
    "- Keeps important features, removes noise\n",
    "- Reduces computation\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Parameter Sharing\n",
    "\n",
    "- Same filter slides across the image\n",
    "- **Drastically fewer weights** than ANN\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Feature              | ANN (Fully Connected)  | CNN (Convolutional) |\n",
    "|----------------------|------------------------|----------------------|\n",
    "| Input shape          | Flattened              | 2D/3D (image)        |\n",
    "| Parameters           | Very high              | Much lower           |\n",
    "| Spatial info         | Lost                   | Preserved            |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981372b-6d30-4d1e-b36e-b8cca1f782fc",
   "metadata": {},
   "source": [
    "# ✅ **Filters (Kernels)and Convolution Operation**\n",
    "\n",
    "---\n",
    "\n",
    "# What are Filters / Kernels?\n",
    "\n",
    "- A **filter** (also called a **kernel**) is a small matrix used to extract features from an image.\n",
    "- Common sizes: 3×3, 5×5, etc.\n",
    "- Each filter detects a specific pattern (e.g., edge, corner, texture).\n",
    "\n",
    "## Example: 3×3 Edge Detection Filter\n",
    "\n",
    "[[-1, -1, -1], [ 0, 0, 0], [ 1, 1, 1]]\n",
    "\n",
    "\n",
    "- This filter highlights **horizontal edges** in an image.\n",
    "\n",
    "---\n",
    "\n",
    "# What is Convolution Operation?\n",
    "\n",
    "- Convolution = sliding the filter over the image and computing **dot products**.\n",
    "- At each position, multiply filter values with the image patch and **sum** the result.\n",
    "\n",
    "## Formula:\n",
    "\n",
    "$$\n",
    "\\text{Output}(i, j) = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} I(i+m, j+n) \\cdot K(m, n)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( I \\) = input image\n",
    "- \\( K \\) = kernel\n",
    "- \\( k \\) = kernel size\n",
    "- \\( (i, j) \\) = position in output feature map\n",
    "\n",
    "---\n",
    "\n",
    "# What is a Convolutional Layer?\n",
    "\n",
    "- A **convolutional layer** applies multiple filters to the input image.\n",
    "- Each filter produces a **feature map**.\n",
    "- The output is a **stack of feature maps** (depth increases).\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "- **Filter**: Small matrix to detect patterns\n",
    "- **Convolution**: Sliding filter over image and computing dot products\n",
    "- **Convolutional Layer**: Applies multiple filters to extract features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc68f2-a3b1-4aba-9e60-d58d47540370",
   "metadata": {},
   "source": [
    "# ✅ **Padding and Strides**\n",
    "\n",
    "---\n",
    "\n",
    "# What is Stride?\n",
    "\n",
    "- **Stride** = number of pixels the filter moves at each step.\n",
    "- Default stride = 1 (moves one pixel at a time).\n",
    "- Larger stride = smaller output feature map.\n",
    "\n",
    "## Example:\n",
    "\n",
    "- Input: 5×5\n",
    "- Filter: 3×3\n",
    "- Stride: 1 → Output: 3×3  \n",
    "- Stride: 2 → Output: 2×2\n",
    "\n",
    "---\n",
    "\n",
    "# What is Padding?\n",
    "\n",
    "- **Padding** = adding extra pixels (usually zeros) around the input image.\n",
    "- Purpose: control output size and preserve edge information.\n",
    "\n",
    "## Types of Padding:\n",
    "\n",
    "| Type         | Description |\n",
    "|--------------|-------------|\n",
    "| **Valid**    | No padding → output shrinks |\n",
    "| **Same**     | Adds padding → output size ≈ input size |\n",
    "| **Custom**   | Manually set padding size |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc5abb-f7fc-4718-8254-a50e13ae78c3",
   "metadata": {},
   "source": [
    "# ✅ **Padding and Stride – Formulas**\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Output Size Formula (1D)\n",
    "\n",
    "For a 1D input of size \\( n \\), kernel size \\( k \\), padding \\( p \\), and stride \\( s \\):\n",
    "\n",
    "$$\n",
    "\\text{Output size} = \\left\\lfloor \\frac{n + 2p - k}{s} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Output Size Formula (2D)\n",
    "\n",
    "For a 2D input of size \\(n*n \\), kernel size \\(k*k \\), padding \\( p \\), and stride \\( s \\):\n",
    "\n",
    "$$\n",
    "\\text{Output height} = \\left\\lfloor \\frac{n + 2p - k}{s} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Output width} = \\left\\lfloor \\frac{n + 2p - k}{s} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Padding to Keep Output Same as Input\n",
    "To keep the output size the same as the input size (when stride = 1):\n",
    "\n",
    "$$\n",
    "p = \\left\\lfloor \\frac{k - 1}{2} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "This is used in **\"same\" padding**.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "- Input size: 32×32  \n",
    "- Kernel size: 3×3  \n",
    "- Stride: 1  \n",
    "- Padding: 1\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Output size} = \\left\\lfloor \\frac{32 + 2(1) - 3}{1} \\right\\rfloor + 1 = 32\n",
    "$$\n",
    "\n",
    "So the output size is **32×32**, same as input.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879a190-e93b-4fb3-aefc-60e1d067e837",
   "metadata": {},
   "source": [
    "# ✅ **Calculating Trainable Parameters in CNN**\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Convolutional Layer\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Parameters} = (F_H \\times F_W \\times C_{\\text{in}} + 1) \\times N_{\\text{filters}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **\\( F_H, F_W \\)**: Filter height and width\n",
    "- **C_in**: Number of input channels\n",
    "- **N_filters**: Number of filters (output channels)\n",
    "- `+1` accounts for the **bias** per filter\n",
    "\n",
    "## Example:\n",
    "\n",
    "- Input: 128 x 128 x 3\n",
    "- Filter size: 3 x 3\n",
    "- Number of filters: 50\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "- Weights per filter: 3 x 3 x 3 = 27\n",
    "- Total weights: 27 x 50 = 1350\n",
    "- Biases: 1 x 50 =50 \n",
    "\n",
    "**Total trainable parameters = 1350 + 50 = 1400**\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Fully Connected (Dense) Layer\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Parameters} = (N_{\\text{in}} + 1) \\times N_{\\text{out}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **N_in**: Number of input units\n",
    "- **N_out**: Number of output units\n",
    "- `+1` is for the bias per output unit\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Pooling Layers\n",
    "\n",
    "- **No trainable parameters**\n",
    "- They only perform downsampling (e.g., max or average pooling)\n",
    "\n",
    "---\n",
    "\n",
    "# Tips\n",
    "\n",
    "| Term               |                 Meaning                 | \n",
    "|--------------------|-----------------------------------------|\n",
    "| Input Channels\t | Depth of input (e.g., 3 for RGB image)   \n",
    "| Output Channels\t | Number of filters used in the layer   \n",
    "| Each Filter Size     | Height×Width×Input Channels\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b5c1a-ccb5-4859-b08b-422e0738cd84",
   "metadata": {},
   "source": [
    "# ✅ **Pooling**\n",
    "\n",
    "---\n",
    "\n",
    "## What is Pooling?\n",
    "\n",
    "- **Pooling** is a downsampling operation used in CNNs.\n",
    "- It reduces the **spatial dimensions** (height and width) of feature maps.\n",
    "- Benefits:\n",
    "  - Reduces computation\n",
    "  - Controls overfitting\n",
    "  - Makes features more robust to translation (position changes)\n",
    "\n",
    "---\n",
    "\n",
    "## How Pooling Works\n",
    "\n",
    "- A small window (e.g., 2×2) slides over the input.\n",
    "- It replaces the window with a **single value** based on a rule (max, average, etc.).\n",
    "- Pooling is applied **independently** to each feature map.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Pooling\n",
    "\n",
    "### 1. Max Pooling\n",
    "\n",
    "- Takes the **maximum** value in each window.\n",
    "- Keeps the most important (strongest) feature.\n",
    "\n",
    "**Example:**\n",
    "Input: [[1, 3], [2, 4]]\n",
    "Max Pooling → 4\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Average Pooling\n",
    "\n",
    "- Takes the **average** of all values in the window.\n",
    "- Smooths the feature map.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Input: [[1, 3], [2, 4]]\n",
    "Average Pooling → (1+2+3+4)/4 = 2.5\n",
    "\n",
    "\n",
    "### 3. Min Pooling\n",
    "\n",
    "- Takes the **minimum** value in each window.\n",
    "- Highlights the **least activated** features.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Input: [[1, 3], [2, 4]]\n",
    "Min Pooling → 1\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Global Pooling\n",
    "\n",
    "- Applies pooling over the **entire feature map**.\n",
    "- Output is a **single value per feature map**.\n",
    "- Used before fully connected layers to reduce dimensions.\n",
    "\n",
    "#### Types:\n",
    "- **Global Max Pooling**: Takes the maximum value of the entire feature map.\n",
    "- **Global Average Pooling**: Takes the average of all values in the feature map.\n",
    "---\n",
    "\n",
    "## Pooling Parameters\n",
    "\n",
    "| Parameter     | Description |\n",
    "|---------------|-------------|\n",
    "| **Window size** | Size of the pooling filter (e.g., 2×2) |\n",
    "| **Stride**      | How far the window moves (usually 2) |\n",
    "| **Padding**     | Usually not used in pooling |\n",
    "\n",
    "---\n",
    "\n",
    "## Output Size Formula (Same as Convolution)\n",
    "\n",
    "For input size \\( n \\), filter size \\( k \\), stride \\( s \\), and padding \\( p \\):\n",
    "\n",
    "$$\n",
    "\\text{Output size} = \\left\\lfloor \\frac{n + 2p - k}{s} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "- Usually, \\( p = 0 \\) and \\( s = k \\) in pooling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b8944-4dd0-4fda-baa6-4a765ff0c83a",
   "metadata": {},
   "source": [
    "# ✅ **CNN Architecture**\n",
    "\n",
    "---\n",
    "\n",
    "# What is a CNN Architecture?\n",
    "\n",
    "- A **CNN architecture** defines how layers are arranged in a convolutional neural network.\n",
    "- It includes:\n",
    "  - Number and type of layers (Conv, Pooling, FC)\n",
    "  - Filter sizes and counts\n",
    "  - Activation functions\n",
    "  - Connections between layers\n",
    "\n",
    "---\n",
    "\n",
    "# Typical Layer Order in a CNN\n",
    "\n",
    "A standard CNN follows this sequence of layers:\n",
    "\n",
    "1. **Input Layer**\n",
    "   - Accepts the image (e.g., 28×28 grayscale or 224×224×3 RGB)\n",
    "\n",
    "2. **Convolutional Layer**\n",
    "   - Applies filters to extract features like edges, textures, etc.\n",
    "\n",
    "3. **Activation Function (ReLU)**\n",
    "   - Adds non-linearity to help the network learn complex patterns\n",
    "4. **Batch Normalization (optional)**\n",
    "   - Normalizes activations to stabilize and speed up training\n",
    "\n",
    "5. **Pooling Layer**\n",
    "   - Downsamples the feature maps to reduce size and computation\n",
    "\n",
    "6. **Dropout Layer (optional)**\n",
    "   - Randomly disables neurons to prevent overfitting\n",
    "\n",
    "7. **Repeat Steps 2–6**\n",
    "   - Multiple convolutional blocks are stacked\n",
    "\n",
    "8. **Flatten Layer**\n",
    "   - Converts 2D feature maps into a 1D vector\n",
    "\n",
    "9. **Fully Connected (Dense) Layer**\n",
    "   - Performs classification based on extracted features\n",
    "\n",
    "10. **Output Layer**\n",
    "    - Final predictions (e.g., softmax for multi-class classification)\n",
    "\n",
    "---\n",
    "\n",
    "# Example Architecture Flow\n",
    "Input → Conv → ReLU → BatchNorm → Pool → Dropout → Conv → ReLU → Pool → Flatten → FC → Output\n",
    "\n",
    "---\n",
    "\n",
    "# Layer Summary\n",
    "\n",
    "| Layer Type         | Purpose                          |\n",
    "|--------------------|----------------------------------|\n",
    "| Convolutional Layer| Feature extraction               |\n",
    "| Activation (ReLU)  | Non-linearity                    |\n",
    "| Batch Norm         | Stabilize training               |\n",
    "| Pooling Layer      | Downsampling                     |\n",
    "| Dropout            | Regularization                   |\n",
    "| Fully Connected    | Classification                   |\n",
    "\n",
    "---\n",
    "\n",
    "# Notes\n",
    "\n",
    "- **BatchNorm** is often placed **after Conv and before ReLU**.\n",
    "- **Dropout** is usually applied **after pooling or before FC layers**.\n",
    "- The exact order may vary slightly in advanced architectures (e.g., ResNet, DenseNet).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Famous CNN Architectures\n",
    "\n",
    "## 1. LeNet-5 (1998)\n",
    "- Designed for digit recognition (MNIST)\n",
    "- Simple: 2 conv layers + 2 FC layers\n",
    "- Input: 32×32 grayscale\n",
    "## 2. AlexNet (2012)\n",
    "- Won ImageNet 2012\n",
    "- 5 conv layers + 3 FC layers\n",
    "- Used ReLU, dropout, and GPU training\n",
    "\n",
    "## 3. VGGNet (2014)\n",
    "- Very deep: 16 or 19 layers\n",
    "- Uses only 3×3 filters\n",
    "- Easy to understand and implement\n",
    "## 4. GoogLeNet (Inception) (2014)\n",
    "- Introduced **Inception modules**\n",
    "- Mixed filters (1×1, 3×3, 5×5) in parallel\n",
    "- Very efficient and deep\n",
    "\n",
    "## 5. ResNet (2015)\n",
    "- Introduced **skip connections** (residual blocks)\n",
    "- Solves vanishing gradient problem\n",
    "- Very deep: up to 152 layers\n",
    "\n",
    "## 6. DenseNet (2017)\n",
    "- Each layer connects to **all previous layers**\n",
    "- Improves feature reuse and gradient flow\n",
    "\n",
    "---\n",
    "\n",
    "# Comparison Table\n",
    "\n",
    "| Model      | Year | Depth | Key Feature             |\n",
    "|------------|------|-------|-------------------------|\n",
    "| LeNet-5    | 1998 | 7     | First CNN for digits    |\n",
    "| AlexNet    | 2012 | 8     | ReLU, dropout, GPU      |\n",
    "| VGGNet     | 2014 | 16/19 | Simple, deep, 3×3 filters|\n",
    "| GoogLeNet  | 2014 | 22    | Inception modules       |\n",
    "| ResNet     | 2015 | 34–152| Residual connections    |\n",
    "| DenseNet   | 2017 | 121+  | Dense connections       |\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "- CNN architectures evolve to improve **accuracy**, **efficiency**, and **training stability**.\n",
    "- Famous models like **ResNet** and **DenseNet** are widely used in modern applications.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a6c7f-74ff-4e14-92ef-5f8a4379ec9a",
   "metadata": {},
   "source": [
    "# ✅ **Backpropagation in CNN**\n",
    "\n",
    "---\n",
    "\n",
    "# What is Backpropagation?\n",
    "\n",
    "- Backpropagation is the process of **updating weights** in a neural network using **gradient descent**.\n",
    "- It works by computing the **gradient of the loss function** with respect to each weight using the **chain rule** of calculus.\n",
    "\n",
    "---\n",
    "\n",
    "# Backpropagation Steps in CNN\n",
    "\n",
    "## 1. Forward Pass\n",
    "- Compute outputs layer by layer:\n",
    "  - Convolution → Activation → Pooling → Fully Connected → Output\n",
    "- Calculate **loss** using a loss function (e.g., cross-entropy)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Backward Pass (Backpropagation)\n",
    "\n",
    "We compute gradients **from output to input**:\n",
    "\n",
    "### a. Output Layer (Fully Connected)\n",
    "\n",
    "For a loss function \\( L \\) and output \\( \\hat{y} \\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( z = W \\cdot x + b \\)\n",
    "- \\( \\hat{y} \\): prediction\n",
    "- \\( W \\): weights\n",
    "- \\( x \\): input to the layer\n",
    "\n",
    "---\n",
    "\n",
    "### b. Activation Function (e.g., ReLU)\n",
    "\n",
    "For ReLU: f(x) = max(0, x)\n",
    "\n",
    "Its derivative:\n",
    "\n",
    "$$\n",
    "f'(x) = \\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Multiply this with the gradient from the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "### c. Convolutional Layer\n",
    "\n",
    "Let:\n",
    "- \\( I \\): input image\n",
    "- \\( K \\): kernel\n",
    "- \\( O \\): output feature map\n",
    "\n",
    "Then:\n",
    "\n",
    "- Gradient w.r.t. kernel:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial K} = I * \\frac{\\partial L}{\\partial O}\n",
    "$$\n",
    "\n",
    "- Gradient w.r.t. input:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial I} = \\text{full convolution of } \\frac{\\partial L}{\\partial O} \\text{ with flipped } K\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### d. Pooling Layer (e.g., Max Pooling)\n",
    "\n",
    "- **Max Pooling**: Gradient is passed only to the **max value** in the window.\n",
    "- **Average Pooling**: Gradient is **evenly distributed** to all values in the window.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Weight Update (Gradient Descent)\n",
    "\n",
    "Update rule:\n",
    "\n",
    "$$\n",
    "W := W - \\eta \\cdot \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\eta \\): learning rate\n",
    "- \\( \\frac{\\partial L}{\\partial W} \\): gradient of loss w.r.t. weights\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "| Step             | Operation                          |\n",
    "|------------------|------------------------------------|\n",
    "| Forward Pass     | Compute outputs and loss           |\n",
    "| Backward Pass    | Compute gradients using chain rule |\n",
    "| Update Weights   | Apply gradient descent             |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Note:\n",
    "- Backpropagation in CNNs is similar to ANNs, but includes **convolution-specific gradient rules**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8676532-1e23-4769-9a08-571aa8cc8872",
   "metadata": {},
   "source": [
    "# ✅ **Transfer Learning**\n",
    "\n",
    "---\n",
    "\n",
    "# What is Transfer Learning?\n",
    "\n",
    "- Transfer Learning is a technique where a model trained on one task is reused for another related task.\n",
    "- In CNNs, it means using a **pre-trained model** (like VGG16, ResNet50) trained on a large dataset (e.g., ImageNet) and adapting it to a new, smaller dataset.\n",
    "\n",
    "---\n",
    "There are two main approaches:\n",
    "\n",
    "## 1. Feature Extraction\n",
    "- Freeze all pre-trained layers.\n",
    "- Train only new classifier layers.\n",
    "\n",
    "## 2. Fine-Tuning\n",
    "- Unfreeze **some deeper layers** of the pre-trained model.\n",
    "- Train both the classifier and a few convolutional layers.\n",
    "\n",
    "---\n",
    "\n",
    "# Why Use Transfer Learning?\n",
    "\n",
    "- Saves **time and computation**.\n",
    "- Useful when you have **limited data**.\n",
    "- Leverages powerful **features learned from large datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "## How Transfer Learning Works\n",
    "\n",
    "### Step 1: Load a Pre-trained CNN\n",
    "- Example models: `VGG16`, `ResNet50`, `MobileNet`.\n",
    "- These are trained on large datasets like **ImageNet** (1.2 million images, 1000 classes).\n",
    "\n",
    "### Step 2: Freeze Early Layers\n",
    "- Early layers learn general visual patterns.\n",
    "- We **freeze** them to keep their weights unchanged.\n",
    "\n",
    "### Step 3: Replace Final Layers\n",
    "- Remove the original classifier (top layers).\n",
    "- Add **custom layers** for your new task.\n",
    "- Example: change output from 1000 classes → 10 classes.\n",
    "\n",
    "### Step 4: Train on New Dataset\n",
    "- Train only the new layers.\n",
    "- Optionally, **fine-tune** some deeper layers later.\n",
    "\n",
    "---\n",
    "\n",
    "# Example (Using Keras)\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Step 1: Load pre-trained model (without top/classifier)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Step 2: Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Step 3: Add new classifier layers\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Step 4: Create new model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
