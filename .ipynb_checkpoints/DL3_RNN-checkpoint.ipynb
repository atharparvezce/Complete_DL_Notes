{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2689942d-2ced-4121-a7f4-3ec5eb62969e",
   "metadata": {},
   "source": [
    "# ✅ **What are RNNs**\n",
    "\n",
    "**Recurrent Neural Networks (RNNs)** are a type of neural network designed to handle **sequential data** such as:\n",
    "- Time series\n",
    "- Text\n",
    "- Audio\n",
    "- Video\n",
    "\n",
    "Unlike standard neural networks, RNNs **have memory**. They remember previous inputs using internal **hidden states**, making them ideal for problems where **order and context** matter.\n",
    "\n",
    "---\n",
    "\n",
    "# Core Idea\n",
    "\n",
    "RNNs process one element at a time from the input sequence and **pass information forward** through hidden states. This allows them to model **temporal dependencies**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# RNN Architecture & Equations\n",
    "\n",
    "Given an input sequence:  \n",
    "$$\n",
    "x_1, x_2, ..., x_T\n",
    "$$\n",
    "\n",
    "At each time step $t$:\n",
    "\n",
    "1. **Hidden state update**  \n",
    "$$\n",
    "h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "2. **Output (optional)**  \n",
    "$$\n",
    "y_t = W_{hy} \\cdot h_t + b_y\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_t$: input at time step $t$\n",
    "- $h_t$: hidden state at time $t$\n",
    "- $W_{xh}, W_{hh}, W_{hy}$: weight matrices\n",
    "- $b_h, b_y$: bias vectors\n",
    "- $\\tanh$: non-linear activation function\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# RNN vs ANN\n",
    "\n",
    "| **Feature**                 | **ANN**                         |     **RNN**                         |\n",
    "|------------------------|----------------------------------|-------------------------------------|\n",
    "| Input type             | Fixed-size                      | Sequential                           |\n",
    "| Memory of past inputs  | None                            |Maintains hidden state                |\n",
    "| Weight sharing         | No                              | Yes (across time steps)              |\n",
    "| Variable input length  | No                              | Yes                                  |\n",
    "| Temporal modeling      | Not supported                   | Supported                            |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785981f3-e7de-4fb0-83f3-35077bded2d8",
   "metadata": {},
   "source": [
    "# ✅ **Forward Propagation in RNN**\n",
    "\n",
    "**Forward propagation** in an RNN means passing the input sequence step by step through the RNN to compute hidden states and outputs.\n",
    "\n",
    "---\n",
    "\n",
    "# Equations\n",
    "\n",
    "At each time step \\( t \\):\n",
    "\n",
    "- Hidden state:\n",
    "$$\n",
    "h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "- Output (optional):\n",
    "$$\n",
    "y_t = W_{hy} \\cdot h_t + b_y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Example (3 time steps)\n",
    "\n",
    "Given \\( x_1, x_2, x_3 \\) and \\( h_0 = 0 \\):\n",
    "\n",
    "1. Compute \\( h1, y1 \\)  \n",
    "2. Compute \\( h2, y2 \\)  \n",
    "3. Compute \\( h3, y3 \\)\n",
    "\n",
    "---\n",
    "\n",
    "# Visualization\n",
    "\n",
    "x1 → [RNN] → h1 → y1\n",
    "\n",
    "x2 → [RNN] → h2 → y2\n",
    "\n",
    "x3 → [RNN] → h3 → y3\n",
    "\n",
    "↑ ↑\n",
    "h1 h2\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "- Uses previous hidden state \\( h_{t-1} \\) and current input \\( x_t \\)\n",
    "- Same weights are shared at each step\n",
    "- Helps model sequences over time\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878061f4-69fd-4367-80cc-fc62560c2e60",
   "metadata": {},
   "source": [
    "# ✅ **Types of RNNs**\n",
    "\n",
    "RNNs can be structured differently based on input-output sequence format.\n",
    "\n",
    "---\n",
    "\n",
    "| Type            | Input        | Output         | Example               |\n",
    "|-----------------|--------------|----------------|------------------------|\n",
    "| One-to-One      | Single       | Single         | Image classification  |\n",
    "| One-to-Many     | Single       | Sequence       | Image captioning      |\n",
    "| Many-to-One     | Sequence     | Single         | Sentiment analysis    |\n",
    "| Many-to-Many    | Sequence     | Sequence       | Translation, NER      |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#  Many-to-Many RNN: Two Types\n",
    "\n",
    "| Type                 | Input Length | Output Length | Example              |\n",
    "|----------------------|--------------|----------------|----------------------|\n",
    "| Fixed-Length         | Same         | Same           | POS tagging, NER     |\n",
    "| Variable-Length      | Same/Diff    | Different      | Translation, Summary |\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2dbf1b-2c2b-4818-9a4c-371bb6a02f9a",
   "metadata": {},
   "source": [
    "# ✅ **Backpropagation in RNNs (BPTT)**\n",
    "\n",
    "Backpropagation in RNNs is called **Backpropagation Through Time (BPTT)**.\n",
    "\n",
    "---\n",
    "\n",
    "# Why ?\n",
    "\n",
    "- RNNs share weights across time steps\n",
    "- Errors are propagated **back through all time steps**\n",
    "- We must compute how **past hidden states affect future losses**\n",
    "\n",
    "---\n",
    "\n",
    "# Steps of BPTT\n",
    "\n",
    "1. Forward pass to compute:\n",
    "   - \\( h_1, h_2, ..., h_T \\)\n",
    "   - \\( y_1, y_2, ..., y_T \\)\n",
    "2. Compute total loss:\n",
    "   $$\n",
    "   \\mathcal{L} = \\sum_{t=1}^{T} \\mathcal{L}_t\n",
    "   $$\n",
    "3. Backward pass:\n",
    "   - Compute gradients w.r.t weights:\n",
    "     - \\( W_{xh}, W_{hh}, W_{hy} \\)\n",
    "   - Accumulate over time\n",
    "   - Update weights\n",
    "\n",
    "---\n",
    "\n",
    "# Challenges\n",
    "\n",
    "- **Vanishing gradients** → can't learn long-term patterns\n",
    "- **Exploding gradients** → unstable training\n",
    "\n",
    "\n",
    "# Solutions:\n",
    "- Gradient clipping\n",
    "- Use LSTM or GRU instead of basic RNN\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb94a9-d496-4984-9993-1449e5a271ef",
   "metadata": {},
   "source": [
    "# ✅ **Problems with Simple RNNs**\n",
    "\n",
    "Simple (vanilla) RNNs have several issues, especially with long sequences:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Vanishing Gradient\n",
    "\n",
    "- Gradients shrink during backpropagation\n",
    "- Weights stop updating → RNN forgets long-term dependencies\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Exploding Gradient\n",
    "\n",
    "- Gradients grow rapidly → causes unstable training and NaNs\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Short-Term Memory\n",
    "\n",
    "- RNNs focus only on recent inputs\n",
    "- Struggle with tasks needing long-range memory\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Hard to Train\n",
    "\n",
    "- Due to unstable gradients\n",
    "- Training becomes slow and ineffective\n",
    "\n",
    "---\n",
    "\n",
    "# Solutions\n",
    "\n",
    "| Problem                | Solution                        |\n",
    "|------------------------|----------------------------------|\n",
    "| Vanishing gradients     | LSTM / GRU, ReLU, LayerNorm      |\n",
    "| Exploding gradients     | Gradient Clipping                |\n",
    "| Short-term memory       | Use gated RNNs (LSTM, GRU)       |\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335d60e-71a6-465c-8eea-0da74417e155",
   "metadata": {},
   "source": [
    "# ✅ **LSTM (Long Short-Term Memory)**\n",
    "\n",
    "LSTM is an advanced RNN that solves the **vanishing gradient problem** and captures **long-term dependencies**.\n",
    "\n",
    "---\n",
    "\n",
    "# Components of an LSTM Cell\n",
    "\n",
    "- **Cell state** \\( C_t \\): long-term memory\n",
    "- **Hidden state** \\( h_t \\): short-term memory\n",
    "- **Gates**:\n",
    "  - Forget gate\n",
    "  - Input gate\n",
    "  - Output gate\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be8157b-fc9b-4c82-8dbe-ce6fdca92734",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/LSTM.png\" style=\"width: 800px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/LSTM.png\" style=\"width: 800px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faecb4d-526b-4b33-a242-c28fdcfb1847",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# LSTM Equations\n",
    "\n",
    "Given input \\( x_t \\), previous hidden state \\( h_{t-1} \\), and cell state \\( C_{t-1} \\):\n",
    "\n",
    "1. **Forget gate**:\n",
    "$$\n",
    "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "\n",
    "2. **Input gate**:\n",
    "$$\n",
    "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "\n",
    "3. **Cell state update**:\n",
    "$$\n",
    "C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "$$\n",
    "\n",
    "4. **Output gate**:\n",
    "$$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "| Symbol        | Meaning                            |\n",
    "|---------------|-------------------------------------|\n",
    "| \\( f_t \\)     | Forget gate                         |\n",
    "| \\( i_t \\)     | Input gate                          |\n",
    "| \\( o_t \\)     | Output gate                         |\n",
    "| \\( \\tilde{C}_t \\) | Candidate cell state            |\n",
    "| \\( C_t \\)     | Cell state (long-term memory)       |\n",
    "| \\( h_t \\)     | Hidden state (short-term memory)    |\n",
    "\n",
    "---\n",
    "\n",
    "# Why LSTM is Better\n",
    "\n",
    "- Handles long sequences well\n",
    "- Reduces vanishing gradient\n",
    "- Remembers important information using gates\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e279564-1606-414b-85b9-033f49b3bdb6",
   "metadata": {},
   "source": [
    "# ✅ **GRU (Gated Recurrent Unit)**\n",
    "\n",
    "GRU is a simpler version of LSTM that also solves the **vanishing gradient problem** using gates — but with **fewer components**.\n",
    "\n",
    "---\n",
    "\n",
    "# Components of GRU\n",
    "\n",
    "- **Update gate** \\( z_t \\): What to keep from the past\n",
    "- **Reset gate** \\( r_t \\): What to forget\n",
    "- **Hidden state** \\( h_t \\): Acts as both memory and output\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495558c1-0814-49c9-a553-370cb87e483e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/GRU.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/GRU.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe00938-eaab-444a-a26b-d5b56dfe88c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# GRU Equations\n",
    "\n",
    "1. **Update gate**:\n",
    "$$\n",
    "z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "$$\n",
    "\n",
    "2. **Reset gate**:\n",
    "$$\n",
    "r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "$$\n",
    "\n",
    "3. **Candidate hidden state**:\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)\n",
    "$$\n",
    "\n",
    "4. **Final hidden state**:\n",
    "$$\n",
    "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Why Use GRU?\n",
    "\n",
    "- Faster and simpler than LSTM\n",
    "- Requires fewer parameters\n",
    "- Performs well in many tasks\n",
    "- Good for smaller datasets and faster inference\n",
    "\n",
    "---\n",
    "\n",
    "# GRU vs LSTM\n",
    "\n",
    "| Feature         | LSTM               | GRU                      |\n",
    "|-----------------|--------------------|--------------------------|\n",
    "| Gates           | 3 (input, forget, output) | 2 (reset, update) |\n",
    "| Cell state      | Yes                | No (merged into hidden)  |\n",
    "| Complexity      | Higher             | Lower                    |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a8125-ccff-463e-8834-bee6b9492c3d",
   "metadata": {},
   "source": [
    "# ✅ **RNN vs LSTM vs GRU**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9397b27a-ffcc-405d-976e-32f6943715c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/RNNvsLSTMvsGRU.png\" style=\"width: 1000px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/RNNvsLSTMvsGRU.png\" style=\"width: 1000px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36ee48-3034-45b3-8480-837cb5eae947",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ✅ **Deep RNNs (Stacked RNNs)**\n",
    "\n",
    "A **Deep RNN** has multiple RNN layers stacked on top of each other.\n",
    "\n",
    "- The output of one layer becomes the input to the next.\n",
    "- Helps in learning **hierarchical and complex sequence patterns**.\n",
    "\n",
    "# Layer Structure\n",
    "\n",
    "$$\n",
    "\\text{Layer 1: } x_1 \\rightarrow h_1 \\rightarrow h_2 \\rightarrow h_3 \\\\ \n",
    "\\text{Layer 2: } h_1 \\rightarrow h_1' \\rightarrow h_2' \\rightarrow h_3'\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Advantages\n",
    "\n",
    "- Captures deeper temporal relationships  \n",
    "- Improves model performance  \n",
    "- Works well for complex tasks like **language modeling**\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **Bidirectional RNNs**\n",
    "\n",
    "A **Bidirectional RNN** processes the input sequence in both **forward and backward** directions using two separate RNNs.\n",
    "\n",
    "- **Forward RNN** processes:\n",
    "\n",
    "  $$\n",
    "  x_1 \\rightarrow x_2 \\rightarrow \\dots \\rightarrow x_T\n",
    "  $$\n",
    "\n",
    "- **Backward RNN** processes:\n",
    "\n",
    "  $$\n",
    "  x_T \\rightarrow x_{T-1} \\rightarrow \\dots \\rightarrow x_1\n",
    "  $$\n",
    "\n",
    "- The final output at time step \\( t \\) is:\n",
    "\n",
    "  $$\n",
    "  h_t = [\\overrightarrow{h_t};\\ \\overleftarrow{h_t}]\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "# Advantages\n",
    "\n",
    "- Uses both **past and future** context  \n",
    "- Improves performance on tasks like:\n",
    "  - Part-of-Speech (POS) tagging\n",
    "  - Named Entity Recognition (NER)\n",
    "  - Machine Translation\n",
    "\n",
    "# Example\n",
    "\n",
    "Sequence:  \n",
    "$$\n",
    "\\text{Input: } x_1\\quad x_2\\quad x_3\\quad x_4 \\\\\n",
    "\\text{Forward: } \\rightarrow\\quad \\rightarrow\\quad \\rightarrow\\quad \\rightarrow \\\\\n",
    "\\text{Backward: } \\leftarrow\\quad \\leftarrow\\quad \\leftarrow\\quad \\leftarrow \\\\\n",
    "\\text{Final Output: } [\\overrightarrow{h_1} \\oplus \\overleftarrow{h_1}],\\ [\\overrightarrow{h_2} \\oplus \\overleftarrow{h_2}],\\ \\dots\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Note:\n",
    "- You can use Deep RNNs and Bidirectional RNNs concept for each types (LSTM and GRU).\n",
    "- Bidirectional LSTM(BiLSTM)\n",
    "- Bidirectional GRU\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
