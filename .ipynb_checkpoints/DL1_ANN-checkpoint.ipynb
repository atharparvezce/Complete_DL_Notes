{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fab2d82-d466-40b8-9067-ae07555ca46f",
   "metadata": {},
   "source": [
    "---\n",
    "# ✅ **What is Deep Learning?**\n",
    "\n",
    "- Deep Learning (DL) is a **subset of Machine Learning (ML)**.\n",
    "- It uses **Artificial Neural Networks (ANNs)** with multiple (deep) layers.\n",
    "- Learns patterns from data by **automatically extracting features**.\n",
    "- Works best with **large datasets** and high computational power (GPUs).\n",
    "- Used in **computer vision**, **speech recognition**, **NLP**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **Machine Learning vs Deep Learning**\n",
    "\n",
    "| Aspect | Machine Learning (ML) | Deep Learning (DL) |\n",
    "|--------|------------------------|---------------------|\n",
    "| **1. Data Dependency** | Works well with **small/medium** data | Needs **large** datasets |\n",
    "| **2. Hardware Dependency** | Runs on **normal CPU** | Needs **GPUs/TPUs** |\n",
    "| **3. Training Time** | **Faster** training | **Slower**, especially for deep nets |\n",
    "| **4. Feature Selection** | Requires **manual** feature engineering | Learns features **automatically** |\n",
    "| **5. Interpretability** | More **interpretable** | Often a **black box** |\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **Why Deep Learning is Successful Now? (Factors Behind Its Growth)**\n",
    "\n",
    "- **Datasets**: Availability of large datasets (e.g., ImageNet, text corpora).\n",
    "- **Hardware**: GPUs/TPUs speed up matrix computations.\n",
    "- **Architecture**: Modern models like CNNs, RNNs, Transformers.\n",
    "- **Frameworks**: Easy-to-use tools like **TensorFlow**, **PyTorch**.\n",
    "- **Community**: Open-source contributions, tutorials, large research community.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32451a59-e602-4644-afcf-5fde27d010c9",
   "metadata": {},
   "source": [
    "# ✅ **What is a Neural Network?**\n",
    "\n",
    "- A Neural Network is a **computational model** inspired by the **human brain**.\n",
    "- It is made up of layers:\n",
    "  - **Input Layer**: Takes the raw features.\n",
    "  - **Hidden Layers**: Process inputs using **weights**, **biases**, and **activation functions**.\n",
    "  - **Output Layer**: Produces the prediction or output.\n",
    "\n",
    "--- \n",
    "\n",
    "# ✅ **Types of Neural Networks**\n",
    "\n",
    "| Type | Description | Common Use |\n",
    "|------|-------------|-------------|\n",
    "| **1. Feedforward Neural Network (FNN)** | Basic structure, unidirectional | Classification, Regression |\n",
    "| **2. Convolutional Neural Network (CNN)** | Works on images, uses filters | Image classification, Object detection |\n",
    "| **3. Recurrent Neural Network (RNN)** | Works on sequences, has memory | Time-series, NLP |\n",
    "| **4. LSTM (Long Short-Term Memory)** | A type of RNN with better memory handling | Language modeling, Text generation |\n",
    "| **5. GAN (Generative Adversarial Network)** | Generator + Discriminator model | Image generation, Deepfakes |\n",
    "| **6. Autoencoder** | Learns compressed representation | Denoising, Dimensionality Reduction |\n",
    "| **7. Transformer** | Based on self-attention | Language models like BERT, GPT |\n",
    "\n",
    "# Note:\n",
    "- FNN is umbrella term for both Perceptron and MLP.\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ **Applications of Deep Learning**\n",
    "- Computer Vision\n",
    "- Natural Language Processing (NLP)\n",
    "- Healthcare\n",
    "- Self-driving Cars\n",
    "- Finance\n",
    "- Generative AI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901f991-fd2b-4a30-996b-bfb343e50533",
   "metadata": {},
   "source": [
    "# ✅ **Perceptron Algorithm**\n",
    "\n",
    "- The Perceptron Algorithm is a **supervised learning** algorithm used for **binary classification**.\n",
    "- It learns a linear decision boundary to separate two classes.\n",
    "- It updates the weights to reduce classification errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6295692d-3539-4fc0-abb4-30855e808159",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/P1.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/P1.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f785e9-6746-4b40-a9e7-6c4d23e63d4c",
   "metadata": {},
   "source": [
    "---\n",
    "# Perceptron Algorithm Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa55c43e-1255-4f84-ba13-e101079966c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/P2.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/P2.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621ecdc-eb91-4c82-936a-57d8ab1c7fa5",
   "metadata": {},
   "source": [
    "---\n",
    "# Perceptron Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb6e317-d971-49d1-a922-a6ef8266d521",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/P3.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/P3.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d8129-9245-4aa0-867f-8045a119f62a",
   "metadata": {},
   "source": [
    "---\n",
    "# Limitation of Perceptron\n",
    "\n",
    "- Works only if the data is linearly separable\n",
    "- Cannot handle XOR-like problems\n",
    "\n",
    "---\n",
    "# Summary\n",
    "\n",
    "- A simple and fast binary classifier\n",
    "- Learns by updating weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28605630-5a91-44cb-916d-a4b842d853f0",
   "metadata": {},
   "source": [
    "---\n",
    "# ✅ **Perceptron Loss Function**\n",
    "\n",
    "- Used to measure errors in classification.\n",
    "- Loss is 0 if prediction is correct.\n",
    "- Loss is positive if prediction is wrong.\n",
    "- Only penalizes misclassified points.\n",
    "- Helps move decision boundary in the right direction.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "The **perceptron loss** for a single training example is:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, b; \\mathbf{x}, y) = \\max(0, -y(\\mathbf{w}^T \\mathbf{x} + b))\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ff940-5f8f-4ccb-b761-9e384ac9edd0",
   "metadata": {},
   "source": [
    "# ✅ **How to Calculate Trainable Parameters in a Neural Network**\n",
    "\n",
    "---\n",
    "\n",
    "## What are Trainable Parameters?\n",
    "\n",
    "- Trainable parameters = **Weights + Biases**\n",
    "- These are updated during training using **backpropagation**\n",
    "- They define what the model learns\n",
    "\n",
    "---\n",
    "\n",
    "## Formula to Calculate Parameters\n",
    "\n",
    "For each fully connected (dense) layer:\n",
    "\n",
    "**Parameters = (Number of inputs)*(Number of neurons) +(Number of neurons)**\n",
    "\n",
    "- First part = **weights**\n",
    "- Second part = **biases**\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Let’s say you have the following network:\n",
    "\n",
    "- Input layer: 4 features  \n",
    "- Hidden layer 1: 5 neurons  \n",
    "- Hidden layer 2: 3 neurons  \n",
    "- Output layer: 1 neuron\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-step Calculation\n",
    "\n",
    "1. Input → Hidden1\n",
    "\n",
    "(4*5) + 5 = 20 + 5 = 25\n",
    "\n",
    "2. Hidden1 → Hidden2\n",
    "\n",
    "(5*3) + 3 = 15 + 3 = 18\n",
    "\n",
    "3. Hidden2 → Output\n",
    "\n",
    "(3*1) + 1 = 3 + 1 = 4\n",
    "\n",
    "---\n",
    "\n",
    "### Total Trainable Parameters:\n",
    "\n",
    "25 + 18 + 4 = 47\n",
    "\n",
    "---\n",
    "\n",
    "**Tip:-**\n",
    "\n",
    "Only **number of features and neurons per layer** matter not the number of training samples.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2e9f4-d3d4-45ee-82de-b2290e908029",
   "metadata": {},
   "source": [
    "# ✅ **Loss Function**\n",
    "\n",
    "Loss function is a method of evaluating how well your algorithm is modelling your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef4dba0-51ca-4fca-ac9e-4f95ef0acc28",
   "metadata": {},
   "source": [
    "---\n",
    "# Types of Loss Function\n",
    "\n",
    "---\n",
    "\n",
    "**1. Mean Squeared Error(MSE)/Squared Loss/L2 Loss**\n",
    "\n",
    "**2. Mean Absolute Error(MAE)/L1 Loss**\n",
    "\n",
    "**3. Huber Loss**\n",
    "\n",
    "**4. Binary Core Entropy/Log Loss**\n",
    "\n",
    "**5. Categorical Cross Entropy**\n",
    "\n",
    "**6. Sparse Categorical Cross Entropy**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0fbc27-6b69-4d6f-95f5-f2e98627b98a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/LF1.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/LF1.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c106608-472d-407a-b579-af81d26e5b17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/LF2.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/LF2.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7430e3b-c00d-401e-aac8-d34afaa524de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/LF3.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/LF3.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6212b36-4866-4f30-af3c-8cee5fd5e9b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/LF4.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/LF4.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ba2aa2-4673-428b-ac85-a9030f689624",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/LF5.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/LF5.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e305a98f-7643-440d-8c4e-dc422b38f453",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/LF6.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/LF6.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990b3ad-ae57-471a-bac9-30b3055f0122",
   "metadata": {},
   "source": [
    "---\n",
    "## Loss Function Summary Table\n",
    "\n",
    "| **Loss Function**                 | **Use Case**                | **Label Format**        | **Activation Function**    |\n",
    "|----------------------------------|-----------------------------|--------------------------|----------------------------|\n",
    "| **Mean Squared Error (MSE)**     | Regression                  | Continuous values        | `None` or `Linear`         |\n",
    "| **Mean Absolute Error (MAE)**    | Regression                  | Continuous values        | `None` or `Linear`         |\n",
    "| **Huber Loss**                   | Regression with outliers    | Continuous values        | `None` or `Linear`         |\n",
    "| **Binary Cross Entropy**         | Binary Classification       | 0 or 1                   | `Sigmoid`                  |\n",
    "| **Categorical Cross Entropy**    | Multi-class Classification  | One-hot encoded vector   | `Softmax`                  |\n",
    "| **Sparse Categorical Cross Entropy** | Multi-class Classification  | Integer class index       | `Softmax`                  |\n",
    "\n",
    "---\n",
    "\n",
    "## Notes:\n",
    "\n",
    "- Use **MSE/MAE** for regression problems.\n",
    "- Use **Cross Entropy** for classification problems.\n",
    "- Choose activation based on output type:\n",
    "  - `Sigmoid` → for **binary classification**\n",
    "  - `Softmax` → for **multi-class classification**\n",
    "  - `Linear` or none → for **regression**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81999e77-d934-4eba-b805-e7b829972833",
   "metadata": {},
   "source": [
    "# ✅ **Gradient Descent**\n",
    "\n",
    "**Gradient Descent** is an optimization algorithm used to minimize a **loss function** by iteratively moving in the direction of the **negative gradient**.\n",
    "\n",
    "---\n",
    "The general update rule is:\n",
    "\n",
    "**θ = θ - α * ∇L(θ)**\n",
    "\n",
    "Where:\n",
    "\n",
    "- θ is the parameter (or weight)\n",
    "- α is the learning rate (step size)\n",
    "- ∇L(θ) is the gradient of the loss function with respect to θ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e716d9-5735-4f16-8a30-c26c7c21f074",
   "metadata": {},
   "source": [
    "# Types of Gradient Descent\n",
    "\n",
    "---\n",
    "\n",
    "**1. Batch Gradeint Descent**\n",
    "\n",
    "**2. Stochastic Gradeint Descent**\n",
    "\n",
    "**3. Mini-Batch Gradeint Descent**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ad7b6-fde4-489e-9414-7425b50f2811",
   "metadata": {},
   "source": [
    "# Summary Table\n",
    "\n",
    "| Type                        | Data Used per Step | Speed     | Stability | Memory Usage  |Formula|\n",
    "|-----------------------------|--------------------|-----------|-----------|---------------|---------------|\n",
    "| Batch Gradient Descent      | All data           | Slow      | High      | High          |θ = θ - α * (1/m) * Σ ∇Lᵢ(θ)\n",
    "| Stochastic Gradient Descent | 1 sample           | Fast      | Low       | Low           |θ = θ - α * ∇Lᵢ(θ)\n",
    "| Mini-Batch Gradient Descent | Small batch        | Moderate  | Moderate  | Moderate      |θ = θ - α * (1/b) * Σ ∇Lᵢ(θ)\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- **m** is the number of training examples\n",
    "\n",
    "- **b** is the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d83c9-0277-4523-9b9e-3c7a5cc860be",
   "metadata": {},
   "source": [
    "---\n",
    "# ✅ **Multi Layer Perceptron**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e308700e-558b-4e8b-bf14-64724ac3b4b2",
   "metadata": {},
   "source": [
    "## MLP Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3b72964-1af7-421d-b680-9424bd5c8161",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/MLP1.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/MLP1.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147e475-9df3-4da4-8320-5a7e3d2cdffb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What is MLP?\n",
    "\n",
    "An **MLP (Multi-Layer Perceptron)** is a type of **feedforward neural network** that consists of:\n",
    "\n",
    "- **Input Layer**\n",
    "- **One or more Hidden Layers**\n",
    "- **Output Layer**\n",
    "\n",
    "Each layer is **fully connected** to the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematics Behind MLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b7a8298-b53c-4cb6-945e-82e74339b45d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/MLP2.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/MLP2.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41390c4c-8bbd-4d25-9a2c-bece5b51d581",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/MLP3.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/MLP3.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c695374e-2bf9-4558-881a-e5a0974576e9",
   "metadata": {},
   "source": [
    "---\n",
    "# ✅ **Forward Propagation**\n",
    "\n",
    "**Forward Propagation** is the process of sending input data through the layers of the neural network to compute the output.\n",
    "\n",
    "It is used:\n",
    "- During **training** (before loss computation)\n",
    "- During **prediction** (after training)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f70722d5-85db-4c59-b716-d2ff6a71733c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/FF1.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/FF1.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4246565-3e06-49ea-bcda-166a91982343",
   "metadata": {},
   "source": [
    "---\n",
    "## Example for 3-layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f2618fb-4729-4625-bcb3-68caea7bc928",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/FF2.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/FF2.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157fc1da-5a9d-4c60-922a-07d30ba3910d",
   "metadata": {},
   "source": [
    "---\n",
    "# ✅ **Backpropagation**\n",
    "\n",
    "**Backpropagation** is the process of calculating **gradients** (partial derivatives) of the **loss function** with respect to **weights and biases**.\n",
    "\n",
    "These gradients are used to **update model parameters** using **Gradient Descent**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f63d74a5-8ab0-4687-a749-cf80cbd7184d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/BB1.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/BB1.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bb7bfbc-a858-46b9-a6a0-cab272da1cae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/BB2.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/BB2.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4775b6fa-eb7e-47d4-a19e-243dbf6667ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/BB3.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/BB3.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d122f93-69b2-4a85-9fda-b79efc4792e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/BB4.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/BB4.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "699a9f38-67f1-47c3-9593-469c92a78833",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/BB5.png\" style=\"width: 500px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/BB5.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70438b0-5cfe-4bd7-9f80-89a76f1efc2a",
   "metadata": {},
   "source": [
    "---\n",
    "# ✅ **MLP Memoization**\n",
    "\n",
    "**MLP Memoization** refers to an optimization technique used in training **Multilayer Perceptrons (MLPs)**. It involves **caching intermediate computations** during the **forward pass**, so they can be **reused during the backward pass (backpropagation)**.\n",
    "\n",
    "This avoids redundant calculations and improves computational efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is Memoization Needed?\n",
    "\n",
    "During **backpropagation**, we compute gradients using the **chain rule**, which requires:\n",
    "\n",
    "- The **activations**\n",
    "- The **linear outputs**\n",
    "\n",
    "These values are already computed during the **forward pass**, so instead of recomputing them, we **store (memoize)** them for reuse.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of Memoization\n",
    "- Faster training\n",
    "- Reduced redundant computation\n",
    "- Lower memory usage (with smart caching strategies)\n",
    "- Improved scalability for deep networks\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "MLP Memoization is a **smart caching strategy** that stores intermediate results during the forward pass to **optimize** the backward pass. It's a key technique in efficient neural network training.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfe088-6272-4d84-80f3-c8590c364c58",
   "metadata": {},
   "source": [
    "# ✅ **Improving Neural Network Performance**\n",
    "\n",
    "---\n",
    "# **1. Fine-tuning Hyperparameters**\n",
    "---\n",
    "\n",
    "## Number of Hidden Layers\n",
    "- More layers can learn more complex patterns.\n",
    "- Too many can lead to overfitting or vanishing gradients.\n",
    "---\n",
    "## Number of Neurons per Layer\n",
    "- More neurons = more learning capacity.\n",
    "- But too many can slow training or cause overfitting.\n",
    "---\n",
    "## Learning Rate\n",
    "- Controls how big the steps are during training.\n",
    "- Too high = unstable learning; too low = slow learning.\n",
    "---\n",
    "## Batch Size\n",
    "- Number of samples processed before updating weights.\n",
    "- Small batch = faster updates, more noise.\n",
    "- Large batch = stable updates, slower learning.\n",
    "---\n",
    "## Epochs\n",
    "- One epoch = one full pass through the training data.\n",
    "- More epochs = more learning, but risk of overfitting.\n",
    "---\n",
    "## Optimizer\n",
    "- Algorithm that updates weights (e.g., SGD, Adam).\n",
    "- Adam is popular for being fast and adaptive.\n",
    "---\n",
    "## Activation Functions\n",
    "- Add non-linearity to the model.\n",
    "- Common ones: ReLU (fast), Sigmoid (can cause vanishing gradients), Tanh.\n",
    "---\n",
    "\n",
    "\n",
    "# **2. Solving Common Neural Network Problems**\n",
    "---\n",
    "## Vanishing Gradients\n",
    "   - Activation Functions  \n",
    "   - Weight Initialization  \n",
    "---\n",
    "## Overfitting\n",
    "   - Reduce Complexity / Increase Data  \n",
    "   - Dropout Layers  \n",
    "   - Regularization (L1 & L2)  \n",
    "   - Early Stopping  \n",
    "---\n",
    "## Normalization\n",
    "   - Normalizing Inputs  \n",
    "   - Batch Normalization  \n",
    "   - Normalizing Activations  \n",
    "---\n",
    "## Optimizers\n",
    "    - Momentum  \n",
    "    - Adagrad  \n",
    "    - RMSprop  \n",
    "    - Adam  \n",
    "    \n",
    "---\n",
    "\n",
    "## Learning Rate Scheduling\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Checking and Clipping\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813d1ad-219e-402c-85c8-ae125c914d29",
   "metadata": {},
   "source": [
    "# ✅ **Vanishing and Exploding Gradient Problems**\n",
    "\n",
    "---\n",
    "\n",
    "## Vanishing Gradient Problem\n",
    "\n",
    "**What is it?**  \n",
    "When training deep neural networks, the gradients (used to update weights) become **very small** as they are backpropagated through layers. This causes **early layers to learn very slowly or not at all**.\n",
    "\n",
    "**Why does it happen?**  \n",
    "In backpropagation, gradients are multiplied layer by layer. If the values are **less than 1**, they shrink:\n",
    "\n",
    "$$\n",
    "\\text{Gradient} = \\frac{\\partial L}{\\partial w} = \\prod_{i=1}^{n} \\frac{\\partial a_i}{\\partial a_{i-1}}\n",
    "$$\n",
    "\n",
    "If each derivative is < 1, the product becomes **very small** as \\( n \\) increases.\n",
    "\n",
    "**Example:**  \n",
    "If each layer multiplies by 0.5 and you have 10 layers:\n",
    "\n",
    "$$\n",
    "0.5^{10} = 0.00098 \\quad \\text{(almost zero)}\n",
    "$$\n",
    "\n",
    "So, the gradient **vanishes**.\n",
    "\n",
    "---\n",
    "\n",
    "## Exploding Gradient Problem\n",
    "\n",
    "**What is it?**  \n",
    "The opposite: gradients become **very large**, causing **unstable training** and huge weight updates.\n",
    "\n",
    "**Why does it happen?**  \n",
    "If the derivatives are **greater than 1**, they grow exponentially:\n",
    "\n",
    "$$\n",
    "\\text{Gradient} = \\prod_{i=1}^{n} \\frac{\\partial a_i}{\\partial a_{i-1}}\n",
    "$$\n",
    "\n",
    "If each derivative is > 1, the product becomes **very large**.\n",
    "\n",
    "**Example:**  \n",
    "If each layer multiplies by 2 and you have 10 layers:\n",
    "\n",
    "$$\n",
    "2^{10} = 1024 \\quad \\text{(very large)}\n",
    "$$\n",
    "\n",
    "So, the gradient **explodes**.\n",
    "\n",
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "- **Vanishing**: Use ReLU activation, Batch Normalization, or architectures like LSTM/GRU or Residual Networks (ResNets).\n",
    "- **Exploding**: Use Gradient Clipping or better weight initialization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f664c6c-65b4-40b7-adea-b83ccd88a658",
   "metadata": {},
   "source": [
    "# ✅ **Early Stopping**\n",
    "\n",
    "**Early Stopping** is a regularization technique used to **prevent overfitting** during training.\n",
    "\n",
    "It stops training **automatically** when the model's performance on a **validation set** starts to get worse, even if training loss is still decreasing.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is it needed?\n",
    "\n",
    "- During training:\n",
    "  - **Training loss ↓** (model fits training data better)\n",
    "  - **Validation loss ↓ then ↑** (model starts overfitting)\n",
    "\n",
    "We stop at the point where **validation loss is lowest**.\n",
    "\n",
    "---\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "1. Split data into **training** and **validation** sets.\n",
    "2. Monitor **validation loss** after each epoch.\n",
    "3. If validation loss **doesn’t improve** for a set number of epochs (called **patience**), stop training.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Let’s say validation loss over epochs looks like this:\n",
    "\n",
    "| Epoch | Validation Loss |\n",
    "|-------|------------------|\n",
    "| 1     | 0.50             |\n",
    "| 2     | 0.40             |\n",
    "| 3     | 0.35             |\n",
    "| 4     | 0.36             |\n",
    "| 5     | 0.38             |\n",
    "\n",
    "If **patience = 2**, training stops after epoch 5 (no improvement for 2 epochs).\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- Prevents overfitting\n",
    "- Saves training time\n",
    "- Simple to implement\n",
    "\n",
    "---\n",
    "\n",
    "## Common Parameters\n",
    "\n",
    "- `monitor`: What to track (e.g., `val_loss`)\n",
    "- `patience`: How many epochs to wait before stopping\n",
    "- `restore_best_weights`: Option to revert to best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f1cdba-8cb0-4d12-af1d-67646ad3bce9",
   "metadata": {},
   "source": [
    "# ✅ **Feature Scaling**\n",
    "\n",
    "---\n",
    "\n",
    "**Feature Scaling** is a technique to **normalize or standardize** input features so they are on a **similar scale**.\n",
    "\n",
    "This helps models **learn faster and better**, especially those that use gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is it important?\n",
    "\n",
    "- Features with different scales (e.g., age in years vs. income in dollars) can **confuse the model**.\n",
    "- Models like neural networks, SVMs, and KNN are **sensitive to feature magnitudes**.\n",
    "- Scaling ensures **fair contribution** of each feature.\n",
    "\n",
    "---\n",
    "\n",
    "## Common Methods\n",
    "\n",
    "### 1. Min-Max Scaling (Normalization)\n",
    "\n",
    "Scales values to a fixed range [0, 1]:\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "$$\n",
    "\n",
    "### 2. Standardization (Z-score Scaling)\n",
    "\n",
    "Centers data around 0 with unit variance:\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- mu = mean of the feature\n",
    "- sigma = standard deviation\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use What?\n",
    "\n",
    "| Method         | Use When...                          |\n",
    "|----------------|--------------------------------------|\n",
    "| Min-Max        | Data is bounded and not Gaussian     |\n",
    "| Standardization| Data is Gaussian or unbounded        |\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Always **fit scaler on training data only**, then apply to test/validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3238cb6-ed88-4512-a58a-94d9eb8d4595",
   "metadata": {},
   "source": [
    "---\n",
    "# ✅**Dropout Layer**\n",
    "\n",
    "---\n",
    "\n",
    "**Dropout** is a regularization technique used to **prevent overfitting** in neural networks.\n",
    "\n",
    "During training, it **randomly \"drops\" (sets to zero)** a fraction of neurons in a layer on each forward pass.\n",
    "\n",
    "---\n",
    "\n",
    "## Why use Dropout?\n",
    "\n",
    "- Forces the network to **not rely too much on specific neurons**\n",
    "- Encourages the model to **learn more robust features**\n",
    "- Acts like training many smaller networks and averaging them\n",
    "\n",
    "---\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "If dropout rate = 0.5, then **50% of neurons** are randomly turned off during each training step.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "\\text{output}_i = \n",
    "\\begin{cases}\n",
    "0 & \\text{with probability } p \\\\\\\\\n",
    "\\frac{a_i}{1 - p} & \\text{with probability } 1 - p\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- ai is the activation of neuron i\n",
    "- p is the dropout rate\n",
    "\n",
    "---\n",
    "\n",
    "## During Inference\n",
    "\n",
    "- **No neurons are dropped**\n",
    "- Outputs are **scaled down** automatically\n",
    "\n",
    "---\n",
    "\n",
    "## Why Dropout is Similar to Random Forests\n",
    "\n",
    "- In **Random Forests**, each tree is trained on a **random subset of features and data**.\n",
    "- In **Dropout**, each forward pass uses a **random subset of neurons**.\n",
    "- Both techniques:\n",
    "  - Reduce overfitting\n",
    "  - Encourage **diversity** in learning\n",
    "  - Combine multiple \"weaker\" models to form a **stronger, more general model**\n",
    "\n",
    "So, Dropout is like training **many smaller neural networks** and averaging their predictions — just like Random Forests average many decision trees.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Disadvantages of Dropout\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Slower Training\n",
    "\n",
    "- Dropout introduces **randomness**, which can make training **slower to converge**.\n",
    "- The model needs more epochs to reach optimal performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Not Always Effective\n",
    "\n",
    "- Dropout works best in **fully connected layers**.\n",
    "- It may not help much (or even hurt) in **convolutional layers** or **recurrent networks** unless carefully tuned.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Increased Training Time\n",
    "\n",
    "- Because neurons are dropped randomly, the model has to **learn multiple redundant paths**.\n",
    "- This increases the **computational cost** during training.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Harder to Tune\n",
    "\n",
    "- Choosing the right **dropout rate** (e.g., 0.2, 0.5) is not straightforward.\n",
    "- Too high → underfitting  \n",
    "  Too low → overfitting\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Not Used During Inference(Testing/Pridiction)\n",
    "\n",
    "- Dropout is **disabled during testing**, which means the model behaves differently during training and inference.\n",
    "- This can cause **inconsistencies** if not handled properly.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. May Not Work Well with Batch Normalization\n",
    "\n",
    "- Dropout and BatchNorm can **interfere** with each other.\n",
    "- Often, one is preferred over the other depending on the architecture.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Example in Code (Keras)\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model.add(Dropout(0.5))  # 50% dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79999450-0cad-488c-beb7-fd163f4fb78c",
   "metadata": {},
   "source": [
    "---\n",
    "# ✅ **Regularization: L1 and L2**\n",
    "\n",
    "---\n",
    "\n",
    "Regularization is a technique to **reduce overfitting** by **penalizing large weights** in the model.\n",
    "\n",
    "It adds a **penalty term** to the loss function to discourage complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## Modified Loss Function\n",
    "\n",
    "Let:\n",
    "- Lo = original loss (e.g., cross-entropy or MSE)\n",
    "- w  = model weights\n",
    "- lambda= regularization strength\n",
    "\n",
    "Then:\n",
    "\n",
    "- **L1 Regularization (Lasso):**\n",
    "\n",
    "  $$\n",
    "  L = L_0 + \\lambda \\sum |w|\n",
    "  $$\n",
    "\n",
    "- **L2 Regularization (Ridge):**\n",
    "\n",
    "  $$\n",
    "  L = L_0 + \\lambda \\sum w^2\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## L1 vs. L2\n",
    "\n",
    "| Feature         | L1 (Lasso)                  | L2 (Ridge)                  |\n",
    "|-----------------|-----------------------------|-----------------------------|\n",
    "| Penalty         | sum|w|                      |sum w^2                      |\n",
    "| Effect          | Shrinks some weights to 0   | Shrinks all weights evenly  |\n",
    "| Use Case        | Feature selection           | General weight decay        |\n",
    "| Sparsity        | Produces sparse models      | Keeps all features          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "- lambda controls the strength of regularization.\n",
    "- Too high → underfitting  \n",
    "  Too low → overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## Example in Code (Keras)\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# L1\n",
    "Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.01))\n",
    "\n",
    "# L2\n",
    "Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea24de3-7dfb-4df9-8827-0d6acdc48e73",
   "metadata": {},
   "source": [
    "---\n",
    "# ✅ **Activation Functions**\n",
    "\n",
    "---\n",
    "\n",
    "- An **activation function** decides whether a neuron should be **activated or not** by applying a mathematical transformation to its input.\n",
    "\n",
    "- It introduces **non-linearity** into the network, allowing it to learn **complex patterns**.\n",
    "\n",
    "- Without activation functions, a neural network would behave like a **linear model**, no matter how many layers it has\n",
    "\n",
    "---\n",
    "## How a Neuron Processes Inputs\n",
    "\n",
    "Each neuron receives inputs, multiplies them by weights, adds a bias, and then passes the result through an **activation function**:\n",
    "\n",
    "$$\n",
    "a = g(w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( x_1, x_2, ..., x_n \\) are the input features  \n",
    "- \\( w_1, w_2, ..., w_n \\) are the corresponding weights  \n",
    "- \\( b \\) is the bias  \n",
    "- \\( g \\) is the activation function  \n",
    "- \\( a \\) is the output of the neuron\n",
    "\n",
    "---\n",
    "**Note**\n",
    "- If the output \\( a \\) is **high**, the neuron is **activated** — it contributes strongly to the next layer.  \n",
    "- If the output \\( a \\) is **low or zero**, the neuron is **not activated** — it contributes little or nothing.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Sigmoid Activation Function\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Range:** (0, 1)\n",
    "\n",
    "**Use Case:** Binary classification (output layer)\n",
    "\n",
    "**Advantages:**\n",
    "- Smooth and differentiable\n",
    "- Outputs can be interpreted as probabilities\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Vanishing gradient** for large |x|\n",
    "- Outputs not zero-centered\n",
    "- Slow convergence\n",
    "\n",
    "---\n",
    "\n",
    "## 2. tanh (Hyperbolic Tangent)\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Range:** (−1, 1)\n",
    "\n",
    "**Use Case:** Hidden layers (better than sigmoid)\n",
    "\n",
    "**Advantages:**\n",
    "- Zero-centered output\n",
    "- Stronger gradients than sigmoid\n",
    "\n",
    "**Disadvantages:**\n",
    "- Still suffers from **vanishing gradient**\n",
    "- Can saturate for large |x|\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ReLU (Rectified Linear Unit)\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "**Range:** [0, ∞)\n",
    "\n",
    "**Use Case:** Most common in hidden layers\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and fast\n",
    "- Solves vanishing gradient (mostly)\n",
    "- Sparse activation (efficient)\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Dying ReLU**: neurons can get stuck at 0\n",
    "- Not zero-centered\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Function | Range     | Zero-Centered  | Vanishing Gradient      | Speed  | Common Use            |\n",
    "|----------|-----------|----------------|-------------------------|--------|-----------------------|\n",
    "| Sigmoid  | (0, 1)    | No             | Yes                     | Slow   | Output layer (binary) |\n",
    "| tanh     | (−1, 1)   | Yes            | Yes (less than sigmoid) | Medium | Hidden layers         |\n",
    "| ReLU     | [0, ∞)    | No             | No                      | Fast   | Hidden layers         |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8cf044-6610-493f-a916-dd2c815dd831",
   "metadata": {},
   "source": [
    "# ✅ **Variants of ReLU and  Dying ReLU Problem**\n",
    "\n",
    "---\n",
    "\n",
    "## What is ReLU?\n",
    "\n",
    "**ReLU (Rectified Linear Unit):**\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "- Outputs 0 if input is negative  \n",
    "- Outputs input if positive\n",
    "\n",
    "---\n",
    "\n",
    "## Dying ReLU Problem\n",
    "\n",
    "- If too many neurons output **0**, they **stop learning** (gradient = 0)  \n",
    "- This happens when weights push inputs into the negative zone **permanently**\n",
    "- **Common Causes:**\n",
    "    - Large negative bias or weights\n",
    "    - High learning rate → pushes weights too far into negative zone\n",
    "    - Poor initialization → many neurons start with negative outputs\n",
    "\n",
    "---\n",
    "\n",
    "## ReLU Variants\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Leaky ReLU**\n",
    "\n",
    "Allows a small slope for negative inputs:\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Typically, **alpha = 0.01**  \n",
    "- Helps avoid dying ReLU\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Parametric ReLU (PReLU)**\n",
    "\n",
    "Like Leaky ReLU, but alpha is **learned** during training:\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "a x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- More flexible than Leaky ReLU\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **ELU (Exponential Linear Unit)**\n",
    "\n",
    "Smooth curve for negative inputs:\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha (e^x - 1) & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Helps with vanishing gradients  \n",
    "- \\( \\alpha \\) is a hyperparameter (e.g., 1.0)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **SELU (Scaled ELU)**\n",
    "\n",
    "Self-normalizing version of ELU:\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\lambda \\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha (e^x - 1) & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Works best with **specific initialization** and **architecture**  \n",
    "- Keeps activations normalized\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Function     | Handles Dying ReLU? | Learnable? | Smooth? | Common Use             |\n",
    "|--------------|---------------------|------------|---------|------------------------|\n",
    "| ReLU         | ❌                  | ❌         | ❌      | Default                |\n",
    "| Leaky ReLU   | ✅ (fixed slope)    | ❌         | ❌      | Simple fix             |\n",
    "| PReLU        | ✅ (learned slope)  | ✅         | ❌      | More flexible          |\n",
    "| ELU          | ✅                  | ❌         | ✅      | Deep networks          |\n",
    "| SELU         | ✅ (self-normalizing)| ❌        | ✅      | Self-normalizing nets  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919bc3d-fab4-4028-a7d9-81936d15358f",
   "metadata": {},
   "source": [
    "---\n",
    "# ✅ **Weight Initialization**\n",
    "\n",
    "Proper weight initialization helps neural networks **converge faster** and **avoid issues** like vanishing or exploding gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is Weight Initialization Important?\n",
    "\n",
    "- Prevents gradients from becoming too small (vanishing) or too large (exploding)  \n",
    "- Helps maintain stable activations and gradients across layers  \n",
    "- Speeds up training and improves performance\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Xavier / Glorot Initialization\n",
    "\n",
    "**Used for:** Sigmoid, tanh activations  \n",
    "**Goal:** Keep the variance of activations and gradients the same across layers.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "Let:\n",
    "- \\( n_{\\text{in}} \\): number of input units  \n",
    "- \\( n_{\\text{out}} \\): number of output units\n",
    "\n",
    "**Uniform:**\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{U} \\left( -\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}} \\right)\n",
    "$$\n",
    "\n",
    "**Normal:**\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N} \\left( 0, \\frac{2}{n_{in} + n_{out}} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. He Initialization\n",
    "\n",
    "**Used for:** ReLU and its variants  \n",
    "**Goal:** Preserve variance of activations in forward pass\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N} \\left( 0, \\frac{2}{n_{in}} \\right)\n",
    "$$\n",
    "\n",
    "- Helps avoid dying ReLU problem  \n",
    "- Allows deeper networks to train effectively\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa69b23-a1ca-466a-961f-5d81fb7c04e5",
   "metadata": {},
   "source": [
    "# ✅ **Batch Normalization**\n",
    "\n",
    "---\n",
    "\n",
    "**Batch Normalization (BatchNorm)** is a technique to:\n",
    "- Normalize the inputs of each layer\n",
    "- Speed up training\n",
    "- Stabilize the learning process\n",
    "\n",
    "---\n",
    "\n",
    "# Why Use It?\n",
    "\n",
    "- Reduces **internal covariate shift**\n",
    "- Allows **higher learning rates**\n",
    "- Helps prevent **vanishing/exploding gradients**\n",
    "- Acts like a **regularizer**\n",
    "\n",
    "---\n",
    "\n",
    "# Step-by-Step Process\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Compute Mean\n",
    "\n",
    "Given a batch of activations:\n",
    "$$x_1, x_2, ..., x_m$$\n",
    "\n",
    "The mean of the batch is computed as:\n",
    "$$\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i$$\n",
    "\n",
    "## Step 2: Compute Variance\n",
    "The variance of the batch is computed as:\n",
    "$$\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2$$\n",
    "\n",
    "## Step 3: Normalize\n",
    "\n",
    "Each activation is normalized using the mean and variance:\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "\n",
    "Where $\\epsilon$ is a small constant added to avoid division by zero.\n",
    "\n",
    "## Step 4: Scale and Shift\n",
    "The normalized activation is then scaled and shifted using learnable parameters $\\gamma$ and $\\beta$:\n",
    "$$y_i = \\gamma \\hat{x}_i + \\beta$$\n",
    "\n",
    "$\\gamma$: learnable scale parameter  \n",
    "$\\beta$: learnable shift parameter\n",
    "\n",
    "These parameters allow the network to recover the original activations if needed.\n",
    "\n",
    "---\n",
    "# Typical Usage\n",
    "\n",
    "- Use **after linear/convolution layer**\n",
    "- Use **before activation function**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579b663-701f-4405-965e-aeb825e9c8a2",
   "metadata": {},
   "source": [
    "# ✅ **Optimizers**\n",
    "\n",
    "\n",
    "Optimizers are algorithms that **adjust weights** of a neural network to **minimize the loss function**.\n",
    "\n",
    "They use gradients (from backpropagation) to decide **how much and in which direction** to update weights.\n",
    "\n",
    "---\n",
    "\n",
    "# Types of Optimizers\n",
    "\n",
    "## 1. **SGD (Stochastic Gradient Descent)**\n",
    "\n",
    "Basic update rule:\n",
    "\n",
    "$$\n",
    "w = w - \\eta \\cdot \\nabla L(w)\n",
    "$$\n",
    "\n",
    "- $\\eta$: learning rate  \n",
    "- $\\nabla L(w)$: gradient of loss w.r.t. weights\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **SGD with Momentum**\n",
    "\n",
    "Adds a velocity term to smooth updates:\n",
    "\n",
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\eta \\nabla L(w)\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = w - v_t\n",
    "$$\n",
    "\n",
    "- $\\gamma$: momentum factor (e.g. 0.9)\n",
    "- Helps escape local minima and dampen oscillations.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **NAG (Nesterov Accelerated Gradient)**\n",
    "\n",
    "Looks ahead before applying gradient:\n",
    "\n",
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\eta \\nabla L(w - \\gamma v_{t-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = w - v_t\n",
    "$$\n",
    "\n",
    "- More accurate updates than vanilla momentum.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **AdaGrad**\n",
    "\n",
    "Adapts learning rate for each parameter:\n",
    "\n",
    "$$\n",
    "G_t = G_{t-1} + \\nabla L(w)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = w - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot \\nabla L(w)\n",
    "$$\n",
    "\n",
    "- Works well for sparse data.  \n",
    "- But learning rate shrinks too much over time.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **RMSProp**\n",
    "\n",
    "Fixes AdaGrad’s issue by using moving average of squared gradients:\n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta) \\cdot (\\nabla L(w))^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = w - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot \\nabla L(w)\n",
    "$$\n",
    "\n",
    "- Good for non-stationary data and deep networks.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "Combines **Momentum + RMSProp**:\n",
    "\n",
    "1. First moment (mean of gradients):\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L(w)\n",
    "$$\n",
    "\n",
    "2. Second moment (variance):\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)(\\nabla L(w))^2\n",
    "$$\n",
    "\n",
    "3. Bias correction:\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "4. Update weights:\n",
    "\n",
    "$$\n",
    "w = w - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t\n",
    "$$\n",
    "\n",
    "- Best all-rounder. Works well in most cases.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Evolution of Optimizers\n",
    "\n",
    "| Optimizer        | Problem / Limitation                                                                 | How Next Optimizer Improved It                                                  |\n",
    "|------------------|---------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|\n",
    "| **SGD**          | - Very slow convergence  <br> - Stuck in local minima  <br> - Oscillates in ravines  | ➤ Momentum added to smooth and accelerate updates                               |\n",
    "| **SGD + Momentum** | - Still overshoots or fluctuates  <br> - Doesn't look ahead while updating         | ➤ NAG adds lookahead (computes gradient before step) for better guidance        |\n",
    "| **NAG**          | - Still uses same learning rate for all parameters                                   | ➤ AdaGrad introduces adaptive learning rates for each parameter                 |\n",
    "| **AdaGrad**      | - Learning rate shrinks too much over time → stops learning                          | ➤ RMSProp uses moving average to prevent learning rate from vanishing           |\n",
    "| **RMSProp**      | - No momentum  <br> - Can still oscillate                                             | ➤ Adam combines momentum + adaptive learning for stable & fast updates          |\n",
    "| **Adam**         | - Sometimes converges to sub-optimal solutions  <br> - May not generalize well        | ➤ Improvements like AdamW, Nadam, etc. (not discussed here) try to fix          |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Optimizer      | Learns Rate? | Uses Momentum?    | Notes                            |\n",
    "| -------------- | ------------ | ----------------- | -------------------------------- |\n",
    "| SGD            | ❌ No         | ❌ No              | Basic gradient descent           |\n",
    "| SGD + Momentum | ❌ No         | ✅ Yes             | Faster convergence               |\n",
    "| NAG            | ❌ No         | ✅ Yes (lookahead) | Better than Momentum             |\n",
    "| AdaGrad        | ✅ Yes        | ❌ No              | Slows down over time             |\n",
    "| RMSProp        | ✅ Yes        | ❌ No              | Good for non-stationary problems |\n",
    "| Adam           | ✅ Yes        | ✅ Yes             | Best default choice              |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba5223c-ac65-4bae-a761-2d5e753be23c",
   "metadata": {},
   "source": [
    "# ✅**Exponentially Weighted Moving Average (EWMA)**\n",
    "\n",
    "EWMA is a technique used to compute a moving average that gives more weight to recent values and less weight to older values.\n",
    "\n",
    "It’s widely used in optimizers (like RMSProp and Adam) to smooth gradients or squared gradients over time.\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta)x_t\n",
    "$$\n",
    "\n",
    "- $x_t$: current value (like gradient or loss)  \n",
    "- $\\beta$: decay rate (e.g. 0.9 or 0.99)  \n",
    "\n",
    "- Keeps more weight on recent values.  \n",
    "- Smooths out noise in training.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
