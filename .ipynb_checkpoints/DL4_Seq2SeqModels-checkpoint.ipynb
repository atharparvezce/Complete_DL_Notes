{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2532fae-3fc9-4be9-b8a2-300318597d04",
   "metadata": {},
   "source": [
    "# ✅ **Encoder–Decoder Architecture**\n",
    "\n",
    "The Encoder–Decoder model is the backbone of many sequence-to-sequence tasks like **machine translation**, **text summarization**, and **speech-to-text**.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Core Idea**\n",
    "\n",
    "We want to **map an input sequence** $x = (x_1, x_2, ..., x_T)$ to an **output sequence** $y = (y_1, y_2, ..., y_{T'})$, where $T$ and $T'$ can be different.\n",
    "\n",
    "- **Encoder**: Reads and compresses the input into a fixed-size context vector.\n",
    "- **Decoder**: Expands that context into the output sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Architecture Overview**\n",
    "Input → [Encoder] → Context Vector → [Decoder] → Output\n",
    "\n",
    "\n",
    "### **Encoder**\n",
    "\n",
    "- Often built with **LSTMs**, **GRUs**, or **Transformers**.\n",
    "- Reads the input tokens step-by-step.\n",
    "- Produces hidden states and a **final context vector** $(h_T, c_T)$ (for LSTM).\n",
    "\n",
    "### **Decoder**\n",
    "\n",
    "- Takes the **context vector** as the initial hidden state.\n",
    "- Predicts the output sequence **one token at a time**.\n",
    "- At each step, uses its previous output as the next input.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Encoder (Math)**\n",
    "\n",
    "Let $x_t$ be the embedding of the $t^{th}$ input token.\n",
    "\n",
    "**For an LSTM encoder:**\n",
    "\n",
    "$$\n",
    "h_t, c_t = \\text{LSTM}_{enc}(x_t, h_{t-1}, c_{t-1})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $h_t$ = hidden state at time $t$\n",
    "- $c_t$ = cell state (memory)\n",
    "- $h_T, c_T$ = final states passed to the decoder\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Decoder (Math)**\n",
    "\n",
    "Let $y_{t-1}$ be the embedding of the previous output token.\n",
    "\n",
    "**For an LSTM decoder:**\n",
    "\n",
    "$$\n",
    "s_t, c_t' = \\text{LSTM}_{dec}(y_{t-1}, s_{t-1}, c_{t-1}')\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $s_t$ = decoder hidden state\n",
    "- $c_t'$ = decoder cell state\n",
    "- Initial states: $s_0 = h_T$, $c_0' = c_T$ (from encoder)\n",
    "\n",
    "The probability of the next token:\n",
    "\n",
    "$$\n",
    "P(y_t | y_{<t}, x) = \\text{Softmax}(W_o \\cdot s_t + b_o)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $W_o, b_o$ are learned output projection parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Training (Teacher Forcing)**\n",
    "\n",
    "During training:\n",
    "\n",
    "- At step $t$, we feed the **ground-truth token** $y_{t-1}$ instead of the predicted one to the decoder.\n",
    "- Loss is **cross-entropy** between predicted probabilities and actual tokens.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{t=1}^{T'} \\log P(y_t^{true} | y_{<t}^{true}, x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Inference (Generation)**\n",
    "\n",
    "- Start with a special `<SOS>` token.\n",
    "- Feed the predicted token back into the decoder for the next step.\n",
    "- Stop at `<EOS>` or max length.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Key Limitations**\n",
    "\n",
    "- **Fixed-size context vector bottleneck**: The encoder must compress all input info into a single vector — hard for long sequences.\n",
    "- **Solution**: Attention mechanisms (introduced later) let the decoder look back at all encoder states.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Visual Summary**\n",
    "\n",
    "**Encoder**:  \n",
    "\n",
    "\n",
    "\\[LSTM] → \n",
    "\n",
    "\\[LSTM] → ... → Final state $(h_T, c_T)$\n",
    "\n",
    "**Decoder**:  \n",
    "Initial state from encoder → Predict token → Feed back → Repeat.\n",
    "\n",
    "---\n",
    "\n",
    "## **Notes:**\n",
    "The Encoder–Decoder architecture converts sequences into sequences by first **encoding meaning** into a hidden representation, then **decoding** it step-by-step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb7279-3b11-4478-89cc-00aad54ff1c9",
   "metadata": {},
   "source": [
    "# ✅ **Attention in Encoder–Decoder Models**\n",
    "\n",
    "## **Why Attention?**\n",
    "\n",
    "- Vanilla Encoder–Decoder squeezes all input info into a **single context vector** → bottleneck for long sequences.\n",
    "- **Attention** allows the decoder to **look at all encoder states** and decide which parts of the input are important at each output step.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Bahdanau Attention (Additive Attention, 2015)**\n",
    "\n",
    "**Key idea**:  \n",
    "At each decoder step $t$, compare the **previous decoder hidden state** $s_{t-1}$ with **each encoder hidden state** $h_i$ to find relevance.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Score (alignment model)**:\n",
    "\n",
    "   $$\n",
    "   e_{t,i} = v_a^\\top \\tanh(W_s s_{t-1} + W_h h_i)\n",
    "   $$\n",
    "\n",
    "2. **Attention weights**:\n",
    "\n",
    "   $$\n",
    "   \\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{k=1}^T \\exp(e_{t,k})}\n",
    "   $$\n",
    "\n",
    "3. **Context vector**:\n",
    "\n",
    "   $$\n",
    "   c_t = \\sum_{i=1}^T \\alpha_{t,i} h_i\n",
    "   $$\n",
    "\n",
    "4. **Decoder update**:  \n",
    "   Use $c_t$ with $y_{t-1}$ to produce $s_t$ and predict $y_t$.\n",
    "\n",
    "**Characteristics**:\n",
    "\n",
    "- Computes scores **before** updating decoder state for step $t$.\n",
    "- “Additive” because score uses addition + nonlinearity.\n",
    "- Good for small to medium hidden sizes.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Luong Attention (Multiplicative Attention, 2015)**\n",
    "\n",
    "**Key idea**:  \n",
    "Simpler scoring, computed **after** the current decoder hidden state $s_t$ is obtained.\n",
    "\n",
    "**Variants of score function**:\n",
    "\n",
    "1. **Dot**:\n",
    "\n",
    "   $$\n",
    "   \\text{score}(s_t, h_i) = s_t^\\top h_i\n",
    "   $$\n",
    "\n",
    "2. **General**:\n",
    "\n",
    "   $$\n",
    "   \\text{score}(s_t, h_i) = s_t^\\top W_a h_i\n",
    "   $$\n",
    "\n",
    "3. **Concat**:\n",
    "\n",
    "   $$\n",
    "   \\text{score}(s_t, h_i) = v_a^\\top \\tanh(W_a [s_t; h_i])\n",
    "   $$\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. Compute score between $s_t$ and each $h_i$.\n",
    "2. Softmax over scores → attention weights $\\alpha_{t,i}$.\n",
    "3. Weighted sum → context vector $c_t$.\n",
    "4. Combine $c_t$ with $s_t$ for output prediction.\n",
    "\n",
    "**Characteristics**:\n",
    "\n",
    "- Faster (especially dot-product version).\n",
    "- Works well for large hidden dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Bahdanau vs. Luong**\n",
    "\n",
    "| Feature                   | Bahdanau (2015)                                                                | Luong (2015)                                                            |\n",
    "|---------------------------|--------------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| **Score type**            | **Additive** — uses a small feedforward neural network (ANN) to compare states | **Multiplicative** — uses dot product or weighted dot product (General) |\n",
    "| **How score is computed** | $e_{t,i} = v_a^\\top \\tanh(W_s s_{t-1} + W_h h_i)$                              | Dot: $s_t^\\top h_i$ <br> General: $s_t^\\top W_a h_i$                    |\n",
    "| **Score timing**          | Before decoder state update ($s_{t-1}$ used)                                   | After decoder state update ($s_t$ used)                                 |\n",
    "| **Computation cost**      | Higher (extra ANN adds parameters & computation)                               | Lower (simple multiplication)                                           |\n",
    "| **Best for**              | Small/medium hidden sizes — more flexible matching                             | Large hidden sizes — faster and efficient                               |\n",
    "\n",
    "---\n",
    "\n",
    "## **Notes:**\n",
    "\n",
    "- **Bahdanau** = more complex, better alignments for small/medium models.\n",
    "- **Luong** = simpler, faster, works well for larger models.\n",
    "- Both replace the fixed context vector with a **dynamic context** per output step, improving Seq2Seq performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c779a54-6c20-4ab0-8ff3-31cdbbc33be7",
   "metadata": {},
   "source": [
    "# ✅ **Transformers**\n",
    "\n",
    "Transformers are a type of deep learning architecture introduced in the 2017 paper *“Attention is All You Need”* by Vaswani et al.  \n",
    "They revolutionized **Natural Language Processing (NLP)** by replacing recurrent architectures (RNNs, LSTMs) with **self-attention mechanisms** that can:\n",
    "\n",
    "- Process all tokens in a sequence **in parallel** (faster training)\n",
    "- Capture **long-range dependencies** between words without losing context\n",
    "- Scale well with large datasets and models\n",
    "\n",
    "**Key idea:**  \n",
    "Instead of processing words one-by-one, Transformers let every word **look at** every other word in the sequence, deciding *how much each should matter* for the current word’s meaning.\n",
    "\n",
    "---\n",
    "## **Main Components of Transformers**\n",
    "\n",
    "Transformers are built from modular components that work together to process and generate sequences. They are as follows:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Self-Attention**\n",
    "\n",
    "- Allows each token to **attend to every other token** in the same sequence.\n",
    "- Enables context-aware representations.\n",
    "- Used in both encoder and decoder.\n",
    "- **Masked Self-Attention** is a variant used in the decoder to prevent future token access.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Multi-Head Attention**\n",
    "\n",
    "- Runs multiple self-attention mechanisms (called \"heads\") **in parallel**.\n",
    "- Each head captures different types of relationships (e.g., syntactic, semantic).\n",
    "- Outputs are concatenated and linearly transformed.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Positional Encoding**\n",
    "\n",
    "- Adds **position information** to token embeddings.\n",
    "- Since self-attention is **order-agnostic**, positional encodings help the model understand token order.\n",
    "- Can be **sinusoidal** (fixed) or **learned**.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Layer Normalization**\n",
    "\n",
    "- Normalizes activations across features.\n",
    "- Helps stabilize training and improve convergence.\n",
    "- Applied before or after sublayers (depending on implementation).\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Masked Self-Attention**\n",
    "\n",
    "- Used in **decoder** during training and inference.\n",
    "- Prevents a token from attending to **future tokens**.\n",
    "- Ensures autoregressive generation (left-to-right prediction).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Cross-Attention**\n",
    "\n",
    "- Used in **decoder** to attend to **encoder outputs**.\n",
    "- Essential for **sequence-to-sequence tasks** like translation.\n",
    "- Helps decoder incorporate source sentence information.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Encoder Architecture**\n",
    "\n",
    "- Composed of multiple identical layers.\n",
    "- Each layer includes:\n",
    "  - Self-Attention\n",
    "  - Feed-Forward Network (FFN)\n",
    "  - Layer Normalization\n",
    "  - Residual Connections\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Decoder Architecture**\n",
    "\n",
    "- Similar to encoder but with additional components:\n",
    "  - **Masked Self-Attention**\n",
    "  - **Cross-Attention** (to encoder outputs)\n",
    "  - Feed-Forward Network\n",
    "  - Layer Normalization\n",
    "  - Residual Connections\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Transformer Inference**\n",
    "\n",
    "- During inference (e.g., text generation):\n",
    "  - Decoder generates tokens **one at a time**.\n",
    "  - Uses **masked self-attention** to prevent future leakage.\n",
    "  - May use techniques like **beam search** or **sampling** for output generation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aad314-887e-4f20-9c0f-240c2ed28db1",
   "metadata": {},
   "source": [
    "# ✅ **1. Self-Attention**\n",
    "\n",
    "Self-attention transforms **input word representations** into **context-aware representations**.  \n",
    "Before we get into it, let’s build up the concepts in the order you mentioned.\n",
    "\n",
    "---\n",
    "\n",
    "## **a. Vectorisation in NLP**\n",
    "\n",
    "We can’t feed text directly to a model — we must **convert words into numerical vectors**.\n",
    "\n",
    "**Common vectorisation methods:**\n",
    "\n",
    "1. **OHE (One-Hot Encoding):**\n",
    "   - Each word gets a unique vector with 1 in one position and 0 elsewhere.\n",
    "   - Problem: extremely sparse, no notion of similarity (e.g., “king” and “queen” are as different as “king” and “banana”).\n",
    "\n",
    "2. **Bag-of-Words / TF-IDF:**\n",
    "   - Counts occurrences of words in a document.\n",
    "   - Loses order/context information.\n",
    "\n",
    "3. **Word2Vec (WoW) and similar distributed representations:**\n",
    "   - Learn dense vector representations where similar words have similar vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## **b. Word Embeddings**\n",
    "\n",
    "- Dense, low-dimensional representations of words.\n",
    "- Capture semantic similarity:\n",
    "\n",
    "  - Example:  \n",
    "    **vec(king) - vec(man) + vec(woman) ≈ vec(queen)**\n",
    "\n",
    "---\n",
    "\n",
    "## **c. Problem of Word Embeddings**\n",
    "\n",
    "- **Static embeddings** (Word2Vec, GloVe) give **one fixed vector per word**.\n",
    "- Meaning is **averaged** across contexts:\n",
    "\n",
    "  - \"Apple\" (fruit) and \"Apple\" (company) get the same embedding.\n",
    "  - This causes ambiguity in downstream tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## **d. Static vs Contextual Embeddings**\n",
    "\n",
    "- **Static Embeddings:** Same vector for a word everywhere.\n",
    "- **Contextual (Dynamic) Embeddings:** Word vectors depend on surrounding words (context).\n",
    "\n",
    "  - Example:  \n",
    "    In \"Apple launched a phone\" vs. \"I ate an apple\", the vector for \"Apple\" will differ.\n",
    "\n",
    "---\n",
    "\n",
    "## **e. General vs Task-Specific Contextual Embeddings**\n",
    "\n",
    "- **General Contextual Embeddings:**  \n",
    "  Produced by large pre-trained models (e.g., BERT, GPT) on massive corpora; can be used for many tasks.\n",
    "\n",
    "- **Task-Specific Embeddings:**  \n",
    "  Fine-tuned on a particular task (e.g., sentiment analysis) so they encode information relevant to that task.\n",
    "\n",
    "---\n",
    "\n",
    "## **f. Why is self attention called self**\n",
    "\n",
    "Self-attention is called “self” because each token (word) in a sequence looks at — or attends to — **other tokens in the same sequence** (including itself) to gather context, rather than attending to a different sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## **g. How Self-Attention Creates Contextual Embeddings**\n",
    "\n",
    "1. Start with **static embeddings** (e.g., from a lookup table).\n",
    "2. Apply **Self-Attention**:\n",
    "   - Each word representation gets updated based on weighted contributions from other words in the sentence.\n",
    "3. The result is a **contextual embedding**:\n",
    "   - Now \"Apple\" in a tech sentence will differ from \"Apple\" in a food sentence.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **h. Scaled Dot-Product Attention (Core math of Self-Attention)**\n",
    "\n",
    "The self-attention mechanism uses three vectors per word:\n",
    "\n",
    "- **Q (Query)** – what this word is looking for\n",
    "- **K (Key)** – what this word offers to others\n",
    "- **V (Value)** – the actual information content\n",
    "\n",
    "**Computation:**\n",
    "\n",
    "1. Compute similarity between Q and K:\n",
    "\n",
    "   $$\n",
    "   \\text{score}(Q, K) = \\frac{Q \\cdot K^T}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "   (Divide by $\\sqrt{d_k}$ to prevent large values that hurt gradient stability.)\n",
    "\n",
    "2. Apply **Softmax** to get attention weights.\n",
    "\n",
    "3. Multiply weights by **V** to get the updated representation.\n",
    "\n",
    "This lets each token decide how much attention to pay to every other token, producing context-rich vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## **Full Process of Self Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a78c2b-7fb7-4eed-881d-3c857c68f4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/T1.png\" style=\"width: 800px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/T1.png\" style=\"width: 800px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c12d2b-2b82-4c4b-91b5-aa7040b9e6f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  **Notes:**\n",
    "\n",
    "- Transformers use **Self-Attention** to build **contextual embeddings** that adapt to the meaning of each word in its sentence.  \n",
    "- This solves the ambiguity problem of static embeddings and enables powerful, scalable NLP models.\n",
    "- Video 76 from the playlist serving as a revision of the encoder–decoder architecture, Bahdanau attention, Luong attention, and self-attention.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8906108-166d-411c-8979-b6557818a5d1",
   "metadata": {},
   "source": [
    "# ✅ **2. Multi-Head Attention**\n",
    "Multi-Head Attention means running **multiple self-attention mechanisms in parallel**, each called a *head*.\n",
    "\n",
    "- Each head learns to focus on different relationships or patterns in the sequence.\n",
    "- The outputs of all heads are **combined** to give a richer, more nuanced representation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Multiple Heads?**\n",
    "\n",
    "A single self-attention layer can learn **one type of relationship** at a time.  \n",
    "Multiple heads allow the model to:\n",
    "\n",
    "- Capture **different kinds of dependencies** between words\n",
    "- Attend to **different positions** and **different aspects** of meaning simultaneously\n",
    "\n",
    "---\n",
    "\n",
    "## **Example**\n",
    "\n",
    "Sentence:\n",
    "\n",
    "> \"The bank can ensure your deposits are safe.\"\n",
    "\n",
    "**Possible interpretations of \"bank\":**\n",
    "\n",
    "- **Head 1**: Focuses on financial meaning — attends to *deposits*, *safe*\n",
    "- **Head 2**: Focuses on grammatical structure — attends to *can ensure*\n",
    "- **Head 3**: Tracks other context cues — maybe *your*, *are*\n",
    "\n",
    "Each head processes its own **Query–Key–Value** attention and returns context-aware vectors.  \n",
    "These vectors are then **concatenated and linearly projected** into a single representation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Analogy**\n",
    "\n",
    "Think of MHA like having a **panel of experts** reading the same sentence:\n",
    "\n",
    "- One expert focuses on **grammar**\n",
    "- Another on **financial context**\n",
    "- Another on **safety/security meaning**\n",
    "\n",
    "After they all share their insights, you **combine** them into a final, comprehensive understanding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ef473b-f171-417f-b7cc-18268007128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/T2.png\" style=\"width: 800px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/T2.png\" style=\"width: 800px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "655b693a-c5d9-470e-a42e-650ab8dca878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"text-align: center;\">\n",
       "    <img src=\"Screenshots/T1.png\" style=\"width: 800px;\"/>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Screenshots/T1.png\" style=\"width: 800px;\"/>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd04470-168a-4fd6-97c7-4b52781bed40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ✅ **3. Positional Encoding**\n",
    "\n",
    "Self-attention in Transformers looks at all tokens in a sequence **simultaneously** — it does **not** know the order of the words by default.  \n",
    "**Positional Encoding** adds information about the position of each token so the model can understand sequence order.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why We Need It**\n",
    "\n",
    "Without positional information:\n",
    "\n",
    "> \"Dog bites man\" and \"Man bites dog\"  \n",
    "> would look the same to the model — just a bag of words.\n",
    "\n",
    "Sequence order is critical for meaning, and positional encoding solves this.\n",
    "\n",
    "---\n",
    "\n",
    "## **How It Works**\n",
    "\n",
    "- A **positional vector** is added to each word embedding.\n",
    "- This positional vector is **unique for each position** in the sequence.\n",
    "- It can be generated using:\n",
    "\n",
    "  - **Fixed sinusoidal functions** (used in the original Transformer paper)\n",
    "  - **Learned position embeddings** (trainable like word embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "## **Example**\n",
    "\n",
    "Sentence:\n",
    "\n",
    "> \"I love NLP\"\n",
    "\n",
    "Let’s say the word embeddings are:\n",
    "\n",
    "- \"I\" → \n",
    "\n",
    "\\[0.5, 0.1, 0.4]  \n",
    "- \"love\" → \n",
    "\n",
    "\\[0.9, 0.3, 0.8]  \n",
    "- \"NLP\" → \n",
    "\n",
    "\\[0.2, 0.7, 0.5]\n",
    "\n",
    "Positional encodings might be:\n",
    "\n",
    "- Position 1 → \n",
    "\n",
    "\\[0.01, 0.99, 0.05]  \n",
    "- Position 2 → \n",
    "\n",
    "\\[0.02, 0.98, 0.10]  \n",
    "- Position 3 → \n",
    "\n",
    "\\[0.03, 0.97, 0.15]\n",
    "\n",
    "**Final input to the model** = word embedding + positional encoding  \n",
    "For example:  \n",
    "\"I\" → \n",
    "\n",
    "\\[0.5 + 0.01, 0.1 + 0.99, 0.4 + 0.05] = \n",
    "\n",
    "\\[0.51, 1.09, 0.45]\n",
    "\n",
    "---\n",
    "\n",
    "## **Analogy**\n",
    "\n",
    "Imagine you have three photos (words) in a pile:\n",
    "\n",
    "- The photos alone tell you *what* is in each picture (semantic meaning).\n",
    "- The captions (positional encodings) tell you *when* each photo was taken (order).\n",
    "\n",
    "Without captions, you could shuffle the pictures and lose the story.\n",
    "\n",
    "---\n",
    "\n",
    "## **Notes**\n",
    "\n",
    "Positional Encoding gives Transformers the ability to **understand word order**, which is essential for capturing meaning in sequences.  \n",
    "It’s a simple yet powerful fix for the order-agnostic nature of self-attention.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c48e5-b7f2-4180-9b83-9656ef67d565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d0cd5-79aa-438a-8e9d-bb8c9497f3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
